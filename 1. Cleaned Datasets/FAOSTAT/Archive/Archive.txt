if not os.path.exists(os.path.join(Cleaned_Datasets_Path, "FAOSTAT_Cleaned_RH.xlsx")):

# Ryan Cleaning:    
    # Load the dataset from the Raw_Datasets_Path directory
    df = pd.read_csv(os.path.join(Raw_Datasets_Path, "FAOSTAT_data.csv"))

    # Convert 'Value' while handling interval strings
    df['Value'] = df['Value'].apply(
        lambda x: safe_float(x) if not isinstance(x, (float, int)) else x
    )

    # Drop irrelevant columns
    df = df.drop(['Domain Code', 'Domain', 'Element Code',
                  'Element', 'Item Code', 'Year Code',
                  'Note', 'Flag'], axis=1)

    # Ensure 'Value' is float
    df['Value'] = df['Value'].apply(lambda x: float(x) if pd.notna(x) else x)

    # Add 'continents' column based on 'Area'
    df['continents'] = df['Area'].apply(lambda x: find_continents(x))

    # Handle missing 'Unit' values
    df['Unit'] = df['Unit'].apply(replace_none)

    # Group by Year, Item, Continent, and Area
    df = df.groupby(['Year', 'Item', 'continents', 'Area']).agg({
        "Flag Description": "first",
        'Unit': 'first',
        'Value': 'mean'
    }).reset_index()

    # Fill missing values by continent-year-item mean
    results = []
    for i in df.loc[df['Value'].isna(), 'Item'].unique():
        filtered = df[df['Item'] == i]
        grouped = filtered.groupby(['Year', 'continents']).agg({
            'Flag Description': 'first',
            'Unit': 'first',
            'Value': 'mean',
            'Item': 'first'
        }).reset_index()
        results.append(grouped)

    means_value_item_one_year = pd.concat(results, ignore_index=True) if results else pd.DataFrame()

    if not means_value_item_one_year.empty:
        df = df.merge(
            means_value_item_one_year[['Year', 'Item', 'continents', 'Value']],
            on=['Year', 'Item', 'continents'],
            how='left',
            suffixes=('', '_patch')
        )
        df['Value'] = df['Value'].fillna(df['Value_patch'])
        df.drop('Value_patch', axis=1, inplace=True)

    # Fill remaining missing values by country-year-item mean
    results = []
    for i in df.loc[df['Value'].isna(), 'Item'].unique():
        filtered = df[df['Item'] == i]
        grouped = filtered.groupby(['Year', 'Area']).agg({
            'Flag Description': 'first',
            'Unit': 'first',
            'Value': 'mean',
            'Item': 'first'
        }).reset_index()
        results.append(grouped)

    means_value_item_one_year = pd.concat(results, ignore_index=True) if results else pd.DataFrame()

    if not means_value_item_one_year.empty:
        df = df.merge(
            means_value_item_one_year[['Year', 'Item', 'Value', 'Area']],
            on=['Year', 'Item', 'Area'],
            how='left',
            suffixes=('', '_patch')
        )
        df['Value'] = df['Value'].fillna(df['Value_patch'])
        df.drop('Value_patch', axis=1, inplace=True)

#Hong Transposing:
    # Drop rows where flag description is "Missing Value", "Missing value; suppressed"
    
    df = df[~df['Flag Description'].isin(["Missing value", "Missing value; suppressed"])]
    
    pivot_df = df.pivot_table(index = ['Year','continents','Area','Flag Description'], columns = 'Item',values= 'Value').reset_index()
    
    # Expand interval years for richer dataset
    
    ## Input is a series
    def expand_year(row):
        y = row["Year"]
        if isinstance(y, str) and "-" in y:
            start, end = map(int, y.split("-"))
            expanded = []
            for yr in range(start, end + 1):
                row_copy = row.copy()
                row_copy["Year"] = str(yr)
                expanded.append(row_copy)
            return expanded
        else:
            return [row]
        
    for _, row in df.iterrows():
        print(row)
        break
    
    # Expand rows
    expanded_rows = []
    for _, row in pivot_df.iterrows():
        expanded_rows.extend(expand_year(row))
    
    # Recreate the DataFrame
    df_expanded = pd.DataFrame(expanded_rows)
    
    # Define columns that are used to identify unique rows
    key_cols = ['Year', 'continents', 'Area','Flag Description']
    
    # Merge rows by taking first non-null for each group
    df_merged = (
        df_expanded
        .groupby(key_cols, as_index=False)
        .first()
    )
    
    conflict_columns = {}
    
    # Check for conflicts
    value_columns = [col for col in df_merged.columns if col not in ['Year', 'Area','continents', 'Flag Description']]
    
    for col in value_columns:
        sub_df = df_merged[[col, 'Flag Description']].dropna(subset=[col])
        flags_present = sub_df['Flag Description'].unique()
        
        if len(flags_present) > 1:
            conflict_columns[col] = flags_present.tolist()
    
    conflicted_cols = list(conflict_columns.keys())
    non_conflicted_cols = [col for col in value_columns if col not in conflicted_cols]
    
    # Define flag priority
    flag_priority = {
        'Official figure': 0,
        'Estimated value': 1,
        'Figure from international organizations': 2 # Not really needed because it does not conflict with other flags
    }
    
    # Assign flag rank
    df_merged['Flag Rank'] = df_merged['Flag Description'].map(flag_priority)
    
    # Keep only necessary columns for conflicted + rank
    df_conflicted = df_merged[['Year', 'continents', 'Area', 'Flag Rank'] + conflicted_cols].copy()
    # df_conflicted = df_conflicted.sort_values(by=['Year', 'continents', 'Area', 'Flag Rank'])
    # df_conflicted_cleaned = df_conflicted.drop_duplicates(subset=['Year', 'continents', 'Area'], keep='first')
    df_conflicted_cleaned = (
        df_conflicted
        .sort_values(by=['Year', 'continents', 'Area', 'Flag Rank'])
        .groupby(['Year', 'continents', 'Area'], as_index=False)
        .first()
    )
    
    # Handle non-conflicted columns using .first()
    df_non_conflicted = (
        df_merged[['Year', 'continents', 'Area'] + non_conflicted_cols]
        .groupby(['Year', 'continents', 'Area'], as_index=False)
        .first()
    )
    
    # Final merge
    df_final = pd.merge(df_non_conflicted, df_conflicted_cleaned, on=['Year', 'continents', 'Area'], how='outer')
    df_final = df_final.drop(columns=['Flag Rank'])
    
    
    group_keys = ['Year', 'continents', 'Area']
    other_cols = sorted([col for col in df_final.columns if col not in group_keys])
    
    df_final1 = df_final[group_keys + other_cols]
    df_final1.head()
    df_final1.to_excel(os.path.join(Cleaned_Datasets_Path, "FAOSTAT_Cleaned_H.xlsx"), index=False)
else:
    print("Cleaned dataset already exists, skipping processing.")
    
    # Save the cleaned DataFrame
    df.to_excel(os.path.join(Cleaned_Datasets_Path, "FAOSTAT_Cleaned_RH.xlsx"), index=False)
else:
    print("Cleaned dataset already exists, skipping processing.")












# Hong Cleaning
# Import the data from the main dataset
FAOSTAT = pd.read_csv("FAOSTAT_data_2025.csv")

# Convert the values of the 'Value' column, accounting for interval data
FAOSTAT['Value'] = FAOSTAT['Value'].apply(
    lambda x: safe_float(x) if not isinstance(x, (float, int)) else x
)

# 3. Drop Irrelevant or Redundant Columns

df = FAOSTAT.drop(['Domain Code', 'Domain', 'Element Code',
                   'Element', 'Item Code',
                   'Year Code', 'Note', 'Flag'], axis=1)

# 4. Convert and Enrich Data

# Convert values to float to ensure consistency
df['Value'] = df['Value'].apply(lambda x: float(x))

# Add continent information based on 'Area'
df['continents'] = df['Area'].apply(lambda x: find_continents(x))

# Replace None values in 'Unit' with a default value like "index"
df['Unit'] = df['Unit'].apply(replace_none)

# 5. Group Data for Imputation Preparation

# Group the data by year, item, continent, and area for aggregation
df = df.groupby(['Year', 'Item', 'continents', 'Area']).agg({
    "Flag Description": "first",
    'Unit': 'first',
    'Value': 'mean'
})
df.reset_index(inplace=True)

# 6. First Stage Imputation (by Year, Item, Continent)

# Create list to store imputed values
results = []

# Loop over each unique item with missing 'Value'
for i in df.loc[df['Value'].isna(), 'Item'].unique():
    filtered = df[df['Item'] == i]
    grouped = filtered.groupby(['Year', 'continents']).agg({
        'Flag Description': 'first',
        'Unit': 'first',
        'Value': 'mean',
        'Item': 'first'
    }).reset_index()
    results.append(grouped)

# Combine all the grouped results into a single DataFrame
means_value_item_one_year = pd.concat(results, ignore_index=True)

# 7. Merge First Imputed Values Back

# Merge the imputed values back to the main dataframe
df = df.merge(
    means_value_item_one_year[['Year', 'Item', 'continents', 'Value']],
    on=['Year', 'Item', 'continents'],
    how='left',
    suffixes=('', '_patch')
)

# Fill missing values with the patch values
df['Value'] = df['Value'].fillna(df_test['Value_patch'])

# Drop the temporary patch column
df.drop('Value_patch', axis=1, inplace=True)

# 8. Second Stage Imputation (by Country and Year)

# Create list for additional imputation
results = []

# Loop over remaining missing items
for i in df.loc[df['Value'].isna(), 'Item'].unique():
    filtered = df[df['Item'] == i]
    
    # Group progressively to get country and year level means
    grouped = filtered.groupby(['Item', 'Area', 'Year', 'continents']).agg({
        'Flag Description': 'first',
        'Unit': 'first',
        'Value': 'mean',
    }).reset_index()

    grouped = grouped.groupby(['Item', 'continents', 'Year']).agg({
        'Flag Description': 'first',
        'Unit': 'first',
        'Value': 'mean',
    }).reset_index()

    grouped = grouped.groupby(['Item', 'Year']).agg({
        'Flag Description': 'first',
        'Unit': 'first',
        'Value': 'mean',
        'continents': 'first'
    }).reset_index()

    results.append(grouped)

# Combine results into single DataFrame
means_value_item_one_year = pd.concat(results, ignore_index=True)

# 9. Merge Secondary Imputed Values into DataFrame

df = df.merge(
    means_value_item_one_year[['Year', 'Item', 'Value', 'continents']],
    on=['Year', 'Item', 'continents'],
    how='left',
    suffixes=('', '_patch')
)

# Fill missing values again
df['Value'] = df['Value'].fillna(df['Value_patch'])
df.drop('Value_patch', axis=1, inplace=True)

# Drop rows that are entirely empty (threshold = 1 non-NA required)
df.dropna(axis=0, inplace=True, thresh=1)

# 10. Filter Out Irrelevant Items and Continents

items_to_drop = [
    'Prevalence of exclusive breastfeeding among infants 0-5 months of age (percent)',
    'Number of children under 5 years affected by wasting (million)',
    'Percentage of children under 5 years affected by wasting (percent)'
]
continent_to_drop = ['South_America', 'Europe', 'North_America']

# Drop the rows based on the item and continent condition
df = df[(~df['Item'].isin(items_to_drop)) | (~df['continents'].isin(continent_to_drop))]


    
    # Drop rows where flag description is "Missing Value", "Missing value; suppressed"
    df = df[~df['Flag Description'].isin(["Missing value", "Missing value; suppressed"])]
    
    pivot_df = df.pivot_table(index = ['Year','continents','Area','Flag Description'], columns = 'Item',values= 'Value').reset_index()
    
    # Expand interval years for richer dataset
    
    ## Input is a series
    def expand_year(row):
        y = row["Year"]
        if isinstance(y, str) and "-" in y:
            start, end = map(int, y.split("-"))
            expanded = []
            for yr in range(start, end + 1):
                row_copy = row.copy()
                row_copy["Year"] = str(yr)
                expanded.append(row_copy)
            return expanded
        else:
            return [row]
        
    for _, row in df.iterrows():
        print(row)
        break
    
    # Expand rows
    expanded_rows = []
    for _, row in pivot_df.iterrows():
        expanded_rows.extend(expand_year(row))
    
    # Recreate the DataFrame
    df_expanded = pd.DataFrame(expanded_rows)
    
    # Define columns that are used to identify unique rows
    key_cols = ['Year', 'continents', 'Area','Flag Description']
    
    # Merge rows by taking first non-null for each group
    df_merged = (
        df_expanded
        .groupby(key_cols, as_index=False)
        .first()
    )
    
    conflict_columns = {}
    
    # Check for conflicts
    value_columns = [col for col in df_merged.columns if col not in ['Year', 'Area','continents', 'Flag Description']]
    
    for col in value_columns:
        sub_df = df_merged[[col, 'Flag Description']].dropna(subset=[col])
        flags_present = sub_df['Flag Description'].unique()
        
        if len(flags_present) > 1:
            conflict_columns[col] = flags_present.tolist()
    
    conflicted_cols = list(conflict_columns.keys())
    non_conflicted_cols = [col for col in value_columns if col not in conflicted_cols]
    
    # Define flag priority
    flag_priority = {
        'Official figure': 0,
        'Estimated value': 1,
        'Figure from international organizations': 2 # Not really needed because it does not conflict with other flags
    }
    
    # Assign flag rank
    df_merged['Flag Rank'] = df_merged['Flag Description'].map(flag_priority)
    
    # Keep only necessary columns for conflicted + rank
    df_conflicted = df_merged[['Year', 'continents', 'Area', 'Flag Rank'] + conflicted_cols].copy()
    # df_conflicted = df_conflicted.sort_values(by=['Year', 'continents', 'Area', 'Flag Rank'])
    # df_conflicted_cleaned = df_conflicted.drop_duplicates(subset=['Year', 'continents', 'Area'], keep='first')
    df_conflicted_cleaned = (
        df_conflicted
        .sort_values(by=['Year', 'continents', 'Area', 'Flag Rank'])
        .groupby(['Year', 'continents', 'Area'], as_index=False)
        .first()
    )
    
    # Handle non-conflicted columns using .first()
    df_non_conflicted = (
        df_merged[['Year', 'continents', 'Area'] + non_conflicted_cols]
        .groupby(['Year', 'continents', 'Area'], as_index=False)
        .first()
    )
    
    # Final merge
    df_final = pd.merge(df_non_conflicted, df_conflicted_cleaned, on=['Year', 'continents', 'Area'], how='outer')
    df_final = df_final.drop(columns=['Flag Rank'])
    
    
    group_keys = ['Year', 'continents', 'Area']
    other_cols = sorted([col for col in df_final.columns if col not in group_keys])
    
    df_final1 = df_final[group_keys + other_cols]
    df_final1.head()
    df_final1.to_excel(os.path.join(Cleaned_Datasets_Path, "FAOSTAT_Cleaned_H.xlsx"), index=False)
else:
    print("Cleaned dataset already exists, skipping processing.")